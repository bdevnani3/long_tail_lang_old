{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb736f95",
   "metadata": {},
   "source": [
    "Computing Projection - sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cad6959f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "images = np.array([[1,1,1], [0,0,0]])\n",
    "\n",
    "text = np.array([[2,2,2], [0,0,0]])\n",
    "\n",
    "proj, _, _, _ = lstsq(text, images)\n",
    "\n",
    "proj\n",
    "# np.dot(text,proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "49b484b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import dataloaders as dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0507887c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /nethome/bdevnani3/flash1/long_tail_lang/data/ImageNet_LT/ImageNet_LT_train.txt\n",
      "Use data transformation: Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.5, 1), ratio=(0.75, 1.3333), interpolation=bicubic)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=None)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      ")\n",
      "***********************DATASET: train random_prompts\n",
      "************************* dict_keys(['default', 'ImageNet', 'bestImageNet'])\n",
      "train 115846\n",
      "No sampler.\n",
      "Shuffle is True.\n"
     ]
    }
   ],
   "source": [
    "# Load Image Data\n",
    "\n",
    "d = dataloader.load_data(\n",
    "    data_root=\"./datasets/ImageNet/\",\n",
    "    dataset=\"ImageNet_LT\",\n",
    "    phase=\"train\",\n",
    "    batch_size=128,\n",
    "#     batch_size=1,\n",
    "    sampler_dic=None,\n",
    "    num_workers=12,\n",
    "    type=\"random_prompts\",\n",
    "    prompt_set=\"ImageNet\",\n",
    ")\n",
    "data = d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6ccb0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# per_class_frequencies = {}\n",
    "\n",
    "# for im, label, _, path in tqdm(data):\n",
    "#     if label.item() not in per_class_frequencies:\n",
    "#         per_class_frequencies[label.item()] = 0\n",
    "#     per_class_frequencies[label.item()] +=1\n",
    "    \n",
    "# json.dump(\"per_class_frequencies.json\", per_class_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "302be98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from clip import clip\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "# Initialize CLIP models \n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "def load_clip_to_cpu(visual_backbone):\n",
    "    backbone_name = visual_backbone\n",
    "    url = clip._MODELS[backbone_name]\n",
    "    model_path = clip._download(url, os.path.expanduser(\"~/.cache/clip\"))\n",
    "\n",
    "    try:\n",
    "        # loading JIT archive\n",
    "        model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "        state_dict = None\n",
    "\n",
    "    except RuntimeError:\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    model = clip.build_model(state_dict or model.state_dict())\n",
    "\n",
    "    return model\n",
    "\n",
    "clip_model = load_clip_to_cpu(\"RN50\")\n",
    "\n",
    "visual_model = torch.nn.DataParallel(clip_model.visual).cuda()\n",
    "\n",
    "text_model = TextEncoder(clip_model)\n",
    "text_model = torch.nn.DataParallel(text_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6f84fc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2970a50ee5c4e41a96c76d2cad0aee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from classes import CLASSES, CUSTOM_TEMPLATES, GENERIC_PROMPT_COLLECTIONS\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "final_images, final_texts = [], []\n",
    "final_labels = []\n",
    "\n",
    "# count = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for im, label, _, path in tqdm(data):\n",
    "        x = visual_model(im.half()).float()\n",
    "        x = x / x.norm(dim=-1, keepdim=True)\n",
    "        final_images.append(x)\n",
    "\n",
    "        templates = np.array(GENERIC_PROMPT_COLLECTIONS[\"ImageNet\"])[path.cpu()]\n",
    "        classnames_for_labels = np.array(CLASSES)[label.cpu()]\n",
    "\n",
    "        texts = clip.tokenize(t.format(c) for t,c in zip(templates, classnames_for_labels))\n",
    "        texts = texts.cuda()\n",
    "        zeroshot_weights = text_model(texts).float()\n",
    "        zeroshot_weights = zeroshot_weights / zeroshot_weights.norm(\n",
    "            dim=-1, keepdim=True\n",
    "        )\n",
    "        final_texts.append(zeroshot_weights)\n",
    "        final_labels.append(label)\n",
    "#         count[label.item] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9ce394c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_images = torch.cat(final_images, dim=0)\n",
    "final_texts = torch.cat(final_texts, dim=0)\n",
    "final_labels = torch.cat(final_labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "87075e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([115846, 1024])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6573de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj, _, _, _ = lstsq(final_texts.cpu(), final_images.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bd6f7a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f012acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"imagenet_text2img_proj.npy\", proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c4eb0",
   "metadata": {},
   "source": [
    "Balanced projection matrix - 1 instance per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "36db2247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c06b3311224ff088274ea83e1d8256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "balanced_images = np.zeros((1000, 1024))\n",
    "balanced_texts = np.zeros((1000, 1024))\n",
    "\n",
    "for i, label in tqdm(enumerate(final_labels)):\n",
    "    balanced_images[label,:] = final_images[i,:].cpu()\n",
    "    balanced_texts[label,:] = final_texts[i,:].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ff7cc62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_proj, _, _, _ = lstsq(balanced_texts, balanced_images)\n",
    "balanced_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f9a092a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"imagenet_text2img_balanced_proj.npy\", balanced_proj.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a60eb",
   "metadata": {},
   "source": [
    "Balanced projection matrix - 100 instances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f3a09223",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_frequencies = {}\n",
    "for label in final_labels:\n",
    "    if label.item() not in label_frequencies:\n",
    "        label_frequencies[label.item()] = 0\n",
    "    label_frequencies[label.item()] += 1\n",
    "\n",
    "indices_by_label = {}\n",
    "\n",
    "for i, label in enumerate(final_labels):\n",
    "    if label.item() not in indices_by_label:\n",
    "        indices_by_label[label.item()] = []\n",
    "    indices_by_label[label.item()].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ee7a4609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfe19e99771446c8e7bad86e8b0c8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "upsampled_images = np.zeros((100000, 1024))\n",
    "upsampled_texts = np.zeros((100000, 1024))\n",
    "\n",
    "for i in tqdm(range(100000)):\n",
    "    label = random.choice(range(1000))\n",
    "    \n",
    "    idx = random.choice(indices_by_label[label])\n",
    "    upsampled_images[i] = final_images[idx].cpu()\n",
    "    upsampled_texts[i] = final_texts[idx].cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "29984ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled_balanced_proj, _, _, _ = lstsq(upsampled_texts, upsampled_images)\n",
    "upsampled_balanced_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "71100bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"imagenet_text2img_upsampled_balanced_proj.npy\", upsampled_balanced_proj.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4a6f939a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75636c0175b14a12a5924382c5a47ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "upsampled_images = np.zeros((400000, 1024))\n",
    "upsampled_texts = np.zeros((400000, 1024))\n",
    "\n",
    "for i in tqdm(range(400000)):\n",
    "    label = random.choice(range(1000))\n",
    "    \n",
    "    idx = random.choice(indices_by_label[label])\n",
    "    upsampled_images[i] = final_images[idx].cpu()\n",
    "    upsampled_texts[i] = final_texts[idx].cpu()\n",
    "\n",
    "upsampled_balanced_proj, _, _, _ = lstsq(upsampled_texts, upsampled_images)\n",
    "upsampled_balanced_proj.shape\n",
    "\n",
    "np.save(\"imagenet_text2img_upsampled_balanced_proj400.npy\", upsampled_balanced_proj.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498010f3",
   "metadata": {},
   "source": [
    "Balanced projection matrix - 100 instances/pooling of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9d457a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9269239a0e4499ba6515d61dd58d732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_labels_text = {}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for label in tqdm(range(1000)):\n",
    "        all_labels_text[label] = []\n",
    "\n",
    "        templates = np.array(GENERIC_PROMPT_COLLECTIONS[\"ImageNet\"])\n",
    "        c = np.array(CLASSES)[label]\n",
    "            \n",
    "        texts = clip.tokenize([template.format(c) for template in templates]) \n",
    "        texts = texts.cuda()\n",
    "        zeroshot_weights = text_model(texts).float()\n",
    "        zeroshot_weights = zeroshot_weights / zeroshot_weights.norm(\n",
    "            dim=-1, keepdim=True\n",
    "        )\n",
    "        all_labels_text[label].append(zeroshot_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "025e648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de7fa772c1247c8a28c05c385ccde4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "upsampled2_images = np.zeros((100000, 1024))\n",
    "upsampled2_texts = np.zeros((100000, 1024))\n",
    "\n",
    "for i in tqdm(range(100000)):\n",
    "    label = random.choice(range(1000))\n",
    "    \n",
    "    idx = random.choice(indices_by_label[label])\n",
    "    upsampled2_images[i] = final_images[idx].cpu()\n",
    "    idx_2 = random.choice(range(82))\n",
    "    upsampled2_texts[i] = all_labels_text[label][0][idx_2].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a1ffbeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1024)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled2_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0c47b7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled2_balanced_proj, _, _, _ = lstsq(upsampled2_texts, upsampled2_images)\n",
    "upsampled2_balanced_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e383a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"imagenet_text2img_upsampled2_balanced_proj.npy\", upsampled2_balanced_proj.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a535d031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a782f46a1c84039aeee0f8fec2bdd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "upsampled2_images = np.zeros((200000, 1024))\n",
    "upsampled2_texts = np.zeros((200000, 1024))\n",
    "\n",
    "for i in tqdm(range(200000)):\n",
    "    label = random.choice(range(1000))\n",
    "    \n",
    "    idx = random.choice(indices_by_label[label])\n",
    "    upsampled2_images[i] = final_images[idx].cpu()\n",
    "    idx_2 = random.choice(range(82))\n",
    "    upsampled2_texts[i] = all_labels_text[label][0][idx_2].cpu()\n",
    "    \n",
    "upsampled2_balanced_proj, _, _, _ = lstsq(upsampled2_texts, upsampled2_images)\n",
    "\n",
    "np.save(\"imagenet_text2img_upsampled2_balanced_proj200.npy\", upsampled2_balanced_proj.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e7db8077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819a0e898fe448d6b2eefb7d7f3d4d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "upsampled2_images = np.zeros((400000, 1024))\n",
    "upsampled2_texts = np.zeros((400000, 1024))\n",
    "\n",
    "for i in tqdm(range(400000)):\n",
    "    label = random.choice(range(1000))\n",
    "    \n",
    "    idx = random.choice(indices_by_label[label])\n",
    "    upsampled2_images[i] = final_images[idx].cpu()\n",
    "    idx_2 = random.choice(range(82))\n",
    "    upsampled2_texts[i] = all_labels_text[label][0][idx_2].cpu()\n",
    "    \n",
    "upsampled2_balanced_proj, _, _, _ = lstsq(upsampled2_texts, upsampled2_images)\n",
    "\n",
    "np.save(\"imagenet_text2img_upsampled2_balanced_proj400.npy\", upsampled2_balanced_proj.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968dc2c9",
   "metadata": {},
   "source": [
    "Balanced projection matrix - 100 instances/pooling of labels/ VL-LTR text pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f6a42912",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_path = \"/nethome/bdevnani3/flash1/long_tail_lang/data_loader/imagenet/wiki/desc_{}.txt\"\n",
    "\n",
    "all_labels_wiki_text = {}\n",
    "all_labels_wiki_text_embs = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for label in range(1000):\n",
    "        all_labels_wiki_text[label] = []\n",
    "        label_desc_path = desc_path.format(label)\n",
    "        f = open(label_desc_path)\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if \"==\" in line:\n",
    "                continue\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            all_labels_wiki_text[label].append(line[:76])\n",
    "        texts = clip.tokenize(all_labels_wiki_text[label])\n",
    "        texts = texts.cuda()\n",
    "        zeroshot_weights = text_model(texts).float()\n",
    "        zeroshot_weights = zeroshot_weights / zeroshot_weights.norm(\n",
    "            dim=-1, keepdim=True\n",
    "        )\n",
    "        all_labels_wiki_text_embs[label] = zeroshot_weights\n",
    "        f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8c5f8c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d80082ec3fd41d39f128979b3366457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "upsampled3_images = np.zeros((100000, 1024))\n",
    "upsampled3_texts = np.zeros((100000, 1024))\n",
    "\n",
    "for i in tqdm(range(100000)):\n",
    "    label = random.choice(range(1000))\n",
    "    \n",
    "    idx = random.choice(indices_by_label[label])\n",
    "    upsampled3_images[i] = final_images[idx].cpu()\n",
    "    idx_2 = random.choice(range(len(all_labels_wiki_text[label])))\n",
    "    upsampled3_texts[i] = all_labels_wiki_text_embs[label][idx_2].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "50ff6d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled3_balanced_proj, _, _, _ = lstsq(upsampled3_texts, upsampled3_images)\n",
    "upsampled3_balanced_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "360cf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"imagenet_text2img_upsampled3_balanced_proj.npy\", upsampled3_balanced_proj.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aaa1a3",
   "metadata": {},
   "source": [
    "Balanced projection matrix - 100 instances/pooling of labels/ VL-LTR text pool + templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4e222881",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_wiki_and_template = {}\n",
    "\n",
    "for label in all_labels_wiki_text_embs:\n",
    "    all_labels_wiki_and_template[label] = torch.cat([all_labels_wiki_text_embs[label], all_labels_text[label][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "935fc10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024d4a4a9bfc42ffacffa538956495c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "upsampled4_images = np.zeros((100000, 1024))\n",
    "upsampled4_texts = np.zeros((100000, 1024))\n",
    "\n",
    "for i in tqdm(range(100000)):\n",
    "    label = random.choice(range(1000))\n",
    "    \n",
    "    idx = random.choice(indices_by_label[label])\n",
    "    upsampled4_images[i] = final_images[idx].cpu()\n",
    "    idx_2 = random.choice(range(len(all_labels_wiki_and_template[label])))\n",
    "    upsampled4_texts[i] = all_labels_wiki_and_template[label][idx_2].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "936020b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled4_balanced_proj, _, _, _ = lstsq(upsampled4_texts, upsampled4_images)\n",
    "upsampled4_balanced_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "929b25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"imagenet_text2img_upsampled4_balanced_proj.npy\", upsampled4_balanced_proj.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac48d15",
   "metadata": {},
   "source": [
    "Project to a combination of both text and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8da257fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39afd0dd7cad4c668b139d00f4053be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "upsampled5_images = np.zeros((100000, 1024))\n",
    "upsampled5_texts = np.zeros((100000, 1024))\n",
    "\n",
    "for i in tqdm(range(100000)):\n",
    "    label = random.choice(range(1000))\n",
    "    \n",
    "    idx_2 = random.choice(range(len(all_labels_wiki_and_template[label])))\n",
    "    upsampled5_texts[i] = all_labels_wiki_and_template[label][idx_2].cpu()\n",
    "    \n",
    "    cointoss = random.choice(range(2))\n",
    "    print(cointoss)\n",
    "    \n",
    "    if cointoss == 0:\n",
    "        idx = random.choice(indices_by_label[label])\n",
    "        upsampled5_images[i] = final_images[idx].cpu()\n",
    "    else:\n",
    "        idx = random.choice(range(len(all_labels_wiki_and_template[label])))\n",
    "        upsampled5_images[i] = all_labels_wiki_and_template[label][idx].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "55ae9443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled5_balanced_proj, _, _, _ = lstsq(upsampled5_texts, upsampled5_images)\n",
    "upsampled5_balanced_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0d68fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"imagenet_text2img_upsampled5_balanced_proj.npy\", upsampled4_balanced_proj.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b6965",
   "metadata": {},
   "source": [
    "Analysis of content of the projection matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "eebb2c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.22019977e+00, -4.05813132e+00,  7.99565807e+00, ...,\n",
       "         3.68203487e+00,  9.65279449e+00,  9.00989232e+00],\n",
       "       [-2.79999779e-01,  2.55104054e+00, -9.46708333e+00, ...,\n",
       "         1.99724974e+00,  1.05673761e+01,  2.01435400e+00],\n",
       "       [ 1.73314683e+00, -4.13588820e+00,  9.67940859e-01, ...,\n",
       "         5.86499522e+00,  1.03482521e+00,  7.07844417e+00],\n",
       "       ...,\n",
       "       [ 4.50658966e-01,  1.25472663e+01, -2.94527356e+00, ...,\n",
       "         2.06302891e+00,  8.13378075e+00,  1.48842194e+01],\n",
       "       [-2.49041376e+00,  6.27473946e+00,  2.70703154e+00, ...,\n",
       "         4.94021511e+00, -4.20177749e+00, -1.76725849e+01],\n",
       "       [-1.51124219e+00,  1.24903615e+00,  3.76061880e-03, ...,\n",
       "         1.99832544e+00,  2.66405994e+00, -9.15591247e+00]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled2_balanced_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f52d3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs = np.linalg.eig(upsampled2_balanced_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "718169fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, v = eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "216dd0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-191.86758+227.50055j)\n",
      "(-191.86758-227.50055j)\n",
      "(-271.64113+7.71438j)\n",
      "(-271.64113-7.71438j)\n",
      "(229.25223+147.61494j)\n",
      "(229.25223-147.61494j)\n",
      "(-108.70222+211.01767j)\n",
      "(-108.70222-211.01767j)\n",
      "(-17.55595+240.04112j)\n",
      "(-17.55595-240.04112j)\n",
      "(-201.09813+87.59422j)\n",
      "(-201.09813-87.59422j)\n",
      "(-185.31961+103.46369j)\n",
      "(-185.31961-103.46369j)\n",
      "(-195.83311+7.4j)\n",
      "(-195.83311-7.4j)\n",
      "(-114.2374+172.51019j)\n",
      "(-114.2374-172.51019j)\n",
      "(-83.38978+193.52439j)\n",
      "(-83.38978-193.52439j)\n",
      "(-177.63156+46.29117j)\n",
      "(-177.63156-46.29117j)\n",
      "(260.30649+0j)\n",
      "(244.26998+65.76284j)\n",
      "(244.26998-65.76284j)\n",
      "(126.80332+192.02814j)\n",
      "(126.80332-192.02814j)\n",
      "(-8.97593+205.56686j)\n",
      "(-8.97593-205.56686j)\n",
      "(155.41452+138.12063j)\n",
      "(155.41452-138.12063j)\n",
      "(12.40778+181.86152j)\n",
      "(12.40778-181.86152j)\n",
      "(212.04363+0j)\n",
      "(185.71186+79.73079j)\n",
      "(185.71186-79.73079j)\n",
      "(50.21728+166.94153j)\n",
      "(50.21728-166.94153j)\n",
      "(7.20089+163.93132j)\n",
      "(7.20089-163.93132j)\n",
      "(93.69756+151.73551j)\n",
      "(93.69756-151.73551j)\n",
      "(187.01059+0j)\n",
      "(134.99519+108.28502j)\n",
      "(134.99519-108.28502j)\n",
      "(117.68911+116.48536j)\n",
      "(117.68911-116.48536j)\n",
      "(165.10977+23.81819j)\n",
      "(165.10977-23.81819j)\n",
      "(-37.24276+151.59548j)\n",
      "(-37.24276-151.59548j)\n",
      "(-123.4413+92.2508j)\n",
      "(-123.4413-92.2508j)\n",
      "(-153.61251+0j)\n",
      "(-73.47525+131.69591j)\n",
      "(-73.47525-131.69591j)\n",
      "(68.33789+127.81954j)\n",
      "(68.33789-127.81954j)\n",
      "(-92.74403+108.94676j)\n",
      "(-92.74403-108.94676j)\n",
      "(-124.80378+64.5696j)\n",
      "(-124.80378-64.5696j)\n",
      "(-49.33942+127.47408j)\n",
      "(-49.33942-127.47408j)\n",
      "(43.45065+127.21024j)\n",
      "(43.45065-127.21024j)\n",
      "(-135.65724+17.70619j)\n",
      "(-135.65724-17.70619j)\n",
      "(77.60185+104.88343j)\n",
      "(77.60185-104.88343j)\n",
      "(113.62853+55.68222j)\n",
      "(113.62853-55.68222j)\n",
      "(101.21595+63.83296j)\n",
      "(101.21595-63.83296j)\n",
      "(113.14743+17.15015j)\n",
      "(113.14743-17.15015j)\n",
      "(21.2752+119.08431j)\n",
      "(21.2752-119.08431j)\n",
      "(109.55605+0j)\n",
      "(96.23834+37.91189j)\n",
      "(96.23834-37.91189j)\n",
      "(-53.10001+97.7743j)\n",
      "(-53.10001-97.7743j)\n",
      "(48.89283+91.46597j)\n",
      "(48.89283-91.46597j)\n",
      "(-14.19865+101.58205j)\n",
      "(-14.19865-101.58205j)\n",
      "(-102.22956+7.58428j)\n",
      "(-102.22956-7.58428j)\n",
      "(67.58868+71.69914j)\n",
      "(67.58868-71.69914j)\n",
      "(-28.14765+94.56307j)\n",
      "(-28.14765-94.56307j)\n",
      "(-93.12719+0j)\n",
      "(9.69611+95.21458j)\n",
      "(9.69611-95.21458j)\n",
      "(-68.7064+64.43389j)\n",
      "(-68.7064-64.43389j)\n",
      "(-83.84775+37.46262j)\n",
      "(-83.84775-37.46262j)\n",
      "(-51.85904+70.82429j)\n",
      "(-51.85904-70.82429j)\n",
      "(-80.71969+25.28746j)\n",
      "(-80.71969-25.28746j)\n",
      "(78.73805+17.12873j)\n",
      "(78.73805-17.12873j)\n",
      "(40.63889+69.47049j)\n",
      "(40.63889-69.47049j)\n",
      "(73.22773+0j)\n",
      "(40.84993+63.67677j)\n",
      "(40.84993-63.67677j)\n",
      "(-40.74167+62.11113j)\n",
      "(-40.74167-62.11113j)\n",
      "(-22.48629+69.5956j)\n",
      "(-22.48629-69.5956j)\n",
      "(-55.07794+45.75843j)\n",
      "(-55.07794-45.75843j)\n",
      "(57.8533+37.51729j)\n",
      "(57.8533-37.51729j)\n",
      "(62.74649+0j)\n",
      "(45.35732+46.50779j)\n",
      "(45.35732-46.50779j)\n",
      "(8.13728+62.60524j)\n",
      "(8.13728-62.60524j)\n",
      "(-32.25485+53.53431j)\n",
      "(-32.25485-53.53431j)\n",
      "(-55.29503+18.32864j)\n",
      "(-55.29503-18.32864j)\n",
      "(11.38722+54.71588j)\n",
      "(11.38722-54.71588j)\n",
      "(-57.40872+0j)\n",
      "(49.95796+17.90362j)\n",
      "(49.95796-17.90362j)\n",
      "(41.44068+22.40378j)\n",
      "(41.44068-22.40378j)\n",
      "(20.75028+42.83561j)\n",
      "(20.75028-42.83561j)\n",
      "(45.97438+0j)\n",
      "(-29.35612+34.75172j)\n",
      "(-29.35612-34.75172j)\n",
      "(-15.92274+42.52826j)\n",
      "(-15.92274-42.52826j)\n",
      "(38.29571+10.87811j)\n",
      "(38.29571-10.87811j)\n",
      "(-35.19854+20.05679j)\n",
      "(-35.19854-20.05679j)\n",
      "(-37.74018+14.1415j)\n",
      "(-37.74018-14.1415j)\n",
      "(18.59049+31.89323j)\n",
      "(18.59049-31.89323j)\n",
      "(-37.15396+0j)\n",
      "(-19.07003+31.25052j)\n",
      "(-19.07003-31.25052j)\n",
      "(1.39066+34.45784j)\n",
      "(1.39066-34.45784j)\n",
      "(-7.85513+31.91698j)\n",
      "(-7.85513-31.91698j)\n",
      "(31.64257+1.92669j)\n",
      "(31.64257-1.92669j)\n",
      "(26.5515+14.27084j)\n",
      "(26.5515-14.27084j)\n",
      "(13.71684+26.65006j)\n",
      "(13.71684-26.65006j)\n",
      "(20.83157+20.2743j)\n",
      "(20.83157-20.2743j)\n",
      "(-23.5808+19.46117j)\n",
      "(-23.5808-19.46117j)\n",
      "(-1.72932+29.25852j)\n",
      "(-1.72932-29.25852j)\n",
      "(27.16328+5.67496j)\n",
      "(27.16328-5.67496j)\n",
      "(-28.71629+0j)\n",
      "(12.95088+21.96526j)\n",
      "(12.95088-21.96526j)\n",
      "(-23.59768+13.65735j)\n",
      "(-23.59768-13.65735j)\n",
      "(-26.47781+0j)\n",
      "(-24.50356+11.43609j)\n",
      "(-24.50356-11.43609j)\n",
      "(7.1949+23.03671j)\n",
      "(7.1949-23.03671j)\n",
      "(22.18064+0j)\n",
      "(2.3805+23.52964j)\n",
      "(2.3805-23.52964j)\n",
      "(-13.16017+18.64147j)\n",
      "(-13.16017-18.64147j)\n",
      "(-19.55342+11.13433j)\n",
      "(-19.55342-11.13433j)\n",
      "(-6.86435+20.45005j)\n",
      "(-6.86435-20.45005j)\n",
      "(-13.58277+16.77175j)\n",
      "(-13.58277-16.77175j)\n",
      "(-19.57396+5.03304j)\n",
      "(-19.57396-5.03304j)\n",
      "(17.70485+9.18684j)\n",
      "(17.70485-9.18684j)\n",
      "(19.44033+1.88607j)\n",
      "(19.44033-1.88607j)\n",
      "(18.64685+0j)\n",
      "(17.6808+7.17701j)\n",
      "(17.6808-7.17701j)\n",
      "(9.05574+16.75937j)\n",
      "(9.05574-16.75937j)\n",
      "(2.72392+18.13247j)\n",
      "(2.72392-18.13247j)\n",
      "(-16.50619+7.76507j)\n",
      "(-16.50619-7.76507j)\n",
      "(-5.30178+17.0656j)\n",
      "(-5.30178-17.0656j)\n",
      "(16.1962+4.87464j)\n",
      "(16.1962-4.87464j)\n",
      "(-12.67529+11.63359j)\n",
      "(-12.67529-11.63359j)\n",
      "(-3.292+16.43592j)\n",
      "(-3.292-16.43592j)\n",
      "(-16.98059+0j)\n",
      "(13.00033+8.85687j)\n",
      "(13.00033-8.85687j)\n",
      "(4.98724+15.32902j)\n",
      "(4.98724-15.32902j)\n",
      "(-14.80481+4.64851j)\n",
      "(-14.80481-4.64851j)\n",
      "(8.52739+11.2633j)\n",
      "(8.52739-11.2633j)\n",
      "(-14.92812+0j)\n",
      "(3.44545+14.02797j)\n",
      "(3.44545-14.02797j)\n",
      "(6.71894+12.25868j)\n",
      "(6.71894-12.25868j)\n",
      "(-1.13044+14.32644j)\n",
      "(-1.13044-14.32644j)\n",
      "(-3.96705+13.51194j)\n",
      "(-3.96705-13.51194j)\n",
      "(10.83833+6.73622j)\n",
      "(10.83833-6.73622j)\n",
      "(11.75272+2.69709j)\n",
      "(11.75272-2.69709j)\n",
      "(-12.44789+6.32645j)\n",
      "(-12.44789-6.32645j)\n",
      "(-0.08996+12.41411j)\n",
      "(-0.08996-12.41411j)\n",
      "(10.75327+0j)\n",
      "(8.35671+7.53736j)\n",
      "(8.35671-7.53736j)\n",
      "(9.61617+4.78936j)\n",
      "(9.61617-4.78936j)\n",
      "(5.43061+9.74533j)\n",
      "(5.43061-9.74533j)\n",
      "(-5.96227+10.50521j)\n",
      "(-5.96227-10.50521j)\n",
      "(-12.03172+0.32998j)\n",
      "(-12.03172-0.32998j)\n",
      "(6.43227+8.79025j)\n",
      "(6.43227-8.79025j)\n",
      "(-3.01059+11.23659j)\n",
      "(-3.01059-11.23659j)\n",
      "(-9.91371+6.07082j)\n",
      "(-9.91371-6.07082j)\n",
      "(-10.89015+2.49386j)\n",
      "(-10.89015-2.49386j)\n",
      "(-3.93154+9.76354j)\n",
      "(-3.93154-9.76354j)\n",
      "(-8.64003+5.99566j)\n",
      "(-8.64003-5.99566j)\n",
      "(-10.01325+3.49361j)\n",
      "(-10.01325-3.49361j)\n",
      "(-6.04716+8.47045j)\n",
      "(-6.04716-8.47045j)\n",
      "(1.89616+10.02959j)\n",
      "(1.89616-10.02959j)\n",
      "(0.38498+9.98843j)\n",
      "(0.38498-9.98843j)\n",
      "(-8.62095+3.60877j)\n",
      "(-8.62095-3.60877j)\n",
      "(8.62184+0j)\n",
      "(8.96046+1.09692j)\n",
      "(8.96046-1.09692j)\n",
      "(6.85939+5.31279j)\n",
      "(6.85939-5.31279j)\n",
      "(7.39741+3.9516j)\n",
      "(7.39741-3.9516j)\n",
      "(2.99887+8.15803j)\n",
      "(2.99887-8.15803j)\n",
      "(7.49634+0j)\n",
      "(6.03072+4.96591j)\n",
      "(6.03072-4.96591j)\n",
      "(4.00292+6.82802j)\n",
      "(4.00292-6.82802j)\n",
      "(7.17108+2.04667j)\n",
      "(7.17108-2.04667j)\n",
      "(5.91692+4.04871j)\n",
      "(5.91692-4.04871j)\n",
      "(6.94275+0.96987j)\n",
      "(6.94275-0.96987j)\n",
      "(-5.45152+6.29805j)\n",
      "(-5.45152-6.29805j)\n",
      "(6.52946+0j)\n",
      "(1.83111+7.27j)\n",
      "(1.83111-7.27j)\n",
      "(-0.91339+7.71009j)\n",
      "(-0.91339-7.71009j)\n",
      "(-6.75521+3.98376j)\n",
      "(-6.75521-3.98376j)\n",
      "(-7.53368+0j)\n",
      "(-3.5914+6.6056j)\n",
      "(-3.5914-6.6056j)\n",
      "(-0.49926+7.02788j)\n",
      "(-0.49926-7.02788j)\n",
      "(3.30508+5.19156j)\n",
      "(3.30508-5.19156j)\n",
      "(1.95195+5.95852j)\n",
      "(1.95195-5.95852j)\n",
      "(-2.28318+6.52399j)\n",
      "(-2.28318-6.52399j)\n",
      "(0.16794+6.37105j)\n",
      "(0.16794-6.37105j)\n",
      "(-6.64419+2.86329j)\n",
      "(-6.64419-2.86329j)\n",
      "(-0.80983+6.01431j)\n",
      "(-0.80983-6.01431j)\n",
      "(-6.10704+2.98761j)\n",
      "(-6.10704-2.98761j)\n",
      "(-6.58156+1.71274j)\n",
      "(-6.58156-1.71274j)\n",
      "(-3.54092+5.28462j)\n",
      "(-3.54092-5.28462j)\n",
      "(-2.34202+5.29264j)\n",
      "(-2.34202-5.29264j)\n",
      "(-4.40284+3.92252j)\n",
      "(-4.40284-3.92252j)\n",
      "(-5.24421+0j)\n",
      "(-3.68986+2.78041j)\n",
      "(-3.68986-2.78041j)\n",
      "(-4.90265+2.46819j)\n",
      "(-4.90265-2.46819j)\n",
      "(-3.14421+3.66203j)\n",
      "(-3.14421-3.66203j)\n",
      "(-5.50093+0.35847j)\n",
      "(-5.50093-0.35847j)\n",
      "(-6.0693+0j)\n",
      "(-3.65811+4.3983j)\n",
      "(-3.65811-4.3983j)\n",
      "(-4.71508+0.76727j)\n",
      "(-4.71508-0.76727j)\n",
      "(-0.1525+5.01163j)\n",
      "(-0.1525-5.01163j)\n",
      "(-4.17534+1.48264j)\n",
      "(-4.17534-1.48264j)\n",
      "(3.22394+4.27924j)\n",
      "(3.22394-4.27924j)\n",
      "(2.11211+4.73084j)\n",
      "(2.11211-4.73084j)\n",
      "(5.20447+0.27086j)\n",
      "(5.20447-0.27086j)\n",
      "(3.68812+3.58902j)\n",
      "(3.68812-3.58902j)\n",
      "(0.82214+4.71529j)\n",
      "(0.82214-4.71529j)\n",
      "(4.52649+2.00507j)\n",
      "(4.52649-2.00507j)\n",
      "(4.73483+1.39465j)\n",
      "(4.73483-1.39465j)\n",
      "(-1.33357+4.06598j)\n",
      "(-1.33357-4.06598j)\n",
      "(3.26424+3.34821j)\n",
      "(3.26424-3.34821j)\n",
      "(2.57615+3.63377j)\n",
      "(2.57615-3.63377j)\n",
      "(3.91881+2.21753j)\n",
      "(3.91881-2.21753j)\n",
      "(3.40843+2.83972j)\n",
      "(3.40843-2.83972j)\n",
      "(1.77578+3.88572j)\n",
      "(1.77578-3.88572j)\n",
      "(0.64277+4.0907j)\n",
      "(0.64277-4.0907j)\n",
      "(4.23158+0.28756j)\n",
      "(4.23158-0.28756j)\n",
      "(4.01299+1.25534j)\n",
      "(4.01299-1.25534j)\n",
      "(-4.16996+0j)\n",
      "(-4.02862+0.81691j)\n",
      "(-4.02862-0.81691j)\n",
      "(-2.18931+3.41684j)\n",
      "(-2.18931-3.41684j)\n",
      "(0.20368+3.89699j)\n",
      "(0.20368-3.89699j)\n",
      "(-2.51775+2.85724j)\n",
      "(-2.51775-2.85724j)\n",
      "(3.67461+0j)\n",
      "(-3.74133+0j)\n",
      "(-3.37487+1.49102j)\n",
      "(-3.37487-1.49102j)\n",
      "(-0.0658+3.5842j)\n",
      "(-0.0658-3.5842j)\n",
      "(3.29784+0j)\n",
      "(-2.5133+2.41555j)\n",
      "(-2.5133-2.41555j)\n",
      "(3.01242+1.68919j)\n",
      "(3.01242-1.68919j)\n",
      "(1.21916+3.20693j)\n",
      "(1.21916-3.20693j)\n",
      "(-1.35247+3.04715j)\n",
      "(-1.35247-3.04715j)\n",
      "(1.75299+2.86077j)\n",
      "(1.75299-2.86077j)\n",
      "(-3.0113+1.47402j)\n",
      "(-3.0113-1.47402j)\n",
      "(3.06791+0.89865j)\n",
      "(3.06791-0.89865j)\n",
      "(0.90321+3.07303j)\n",
      "(0.90321-3.07303j)\n",
      "(2.19666+2.14627j)\n",
      "(2.19666-2.14627j)\n",
      "(0.4633+3.06489j)\n",
      "(0.4633-3.06489j)\n",
      "(-0.22892+2.90944j)\n",
      "(-0.22892-2.90944j)\n",
      "(2.35122+1.61923j)\n",
      "(2.35122-1.61923j)\n",
      "(-2.0562+2.11705j)\n",
      "(-2.0562-2.11705j)\n",
      "(-0.73194+2.659j)\n",
      "(-0.73194-2.659j)\n",
      "(-2.69201+0.42394j)\n",
      "(-2.69201-0.42394j)\n",
      "(-1.08135+2.40781j)\n",
      "(-1.08135-2.40781j)\n",
      "(2.26305+1.22871j)\n",
      "(2.26305-1.22871j)\n",
      "(-1.91602+1.73006j)\n",
      "(-1.91602-1.73006j)\n",
      "(-2.25524+1.26388j)\n",
      "(-2.25524-1.26388j)\n",
      "(-0.40034+2.42661j)\n",
      "(-0.40034-2.42661j)\n",
      "(-2.26737+0.28097j)\n",
      "(-2.26737-0.28097j)\n",
      "(-1.20473+1.94732j)\n",
      "(-1.20473-1.94732j)\n",
      "(-0.21692+2.30519j)\n",
      "(-0.21692-2.30519j)\n",
      "(2.10176+1.03952j)\n",
      "(2.10176-1.03952j)\n",
      "(-2.06951+0.50492j)\n",
      "(-2.06951-0.50492j)\n",
      "(1.58209+1.64173j)\n",
      "(1.58209-1.64173j)\n",
      "(2.18551+0j)\n",
      "(2.21635+0.27631j)\n",
      "(2.21635-0.27631j)\n",
      "(0.52328+2.0731j)\n",
      "(0.52328-2.0731j)\n",
      "(1.33776+1.65346j)\n",
      "(1.33776-1.65346j)\n",
      "(2.00952+0j)\n",
      "(0.73656+1.96622j)\n",
      "(0.73656-1.96622j)\n",
      "(0.84552+1.89441j)\n",
      "(0.84552-1.89441j)\n",
      "(2.06834+0.18972j)\n",
      "(2.06834-0.18972j)\n",
      "(0.22281+1.94j)\n",
      "(0.22281-1.94j)\n",
      "(-1.66598+0.82776j)\n",
      "(-1.66598-0.82776j)\n",
      "(-1.51304+0.84481j)\n",
      "(-1.51304-0.84481j)\n",
      "(-0.80307+1.55349j)\n",
      "(-0.80307-1.55349j)\n",
      "(1.64515+0.5242j)\n",
      "(1.64515-0.5242j)\n",
      "(-1.53568+0j)\n",
      "(1.19188+1.22029j)\n",
      "(1.19188-1.22029j)\n",
      "(-1.07709+1.26527j)\n",
      "(-1.07709-1.26527j)\n",
      "(1.25268+1.00509j)\n",
      "(1.25268-1.00509j)\n",
      "(0.02032+1.51075j)\n",
      "(0.02032-1.51075j)\n",
      "(-1.36977+0j)\n",
      "(-1.14841+0.74248j)\n",
      "(-1.14841-0.74248j)\n",
      "(0.21163+1.34405j)\n",
      "(0.21163-1.34405j)\n",
      "(1.31695+0.23646j)\n",
      "(1.31695-0.23646j)\n",
      "(-0.26099+1.29942j)\n",
      "(-0.26099-1.29942j)\n",
      "(0.36192+1.20358j)\n",
      "(0.36192-1.20358j)\n",
      "(0.73055+1.01445j)\n",
      "(0.73055-1.01445j)\n",
      "(-0.62867+1.03616j)\n",
      "(-0.62867-1.03616j)\n",
      "(-0.42683+1.093j)\n",
      "(-0.42683-1.093j)\n",
      "(-1.05255+0.42868j)\n",
      "(-1.05255-0.42868j)\n",
      "(0.89449+0.69777j)\n",
      "(0.89449-0.69777j)\n",
      "(-1.03498+0.20368j)\n",
      "(-1.03498-0.20368j)\n",
      "(-0.85663+0.60118j)\n",
      "(-0.85663-0.60118j)\n",
      "(0.88108+0.49922j)\n",
      "(0.88108-0.49922j)\n",
      "(0.03882+0.9756j)\n",
      "(0.03882-0.9756j)\n",
      "(0.31654+0.86485j)\n",
      "(0.31654-0.86485j)\n",
      "(0.64559+0.60955j)\n",
      "(0.64559-0.60955j)\n",
      "(0.89514+0j)\n",
      "(0.83151+0j)\n",
      "(-0.66993+0.36527j)\n",
      "(-0.66993-0.36527j)\n",
      "(-0.75915+0.01659j)\n",
      "(-0.75915-0.01659j)\n",
      "(-0.38914+0.63163j)\n",
      "(-0.38914-0.63163j)\n",
      "(0.74355+0.21921j)\n",
      "(0.74355-0.21921j)\n",
      "(0.08059+0.69317j)\n",
      "(0.08059-0.69317j)\n",
      "(0.71272+0.08182j)\n",
      "(0.71272-0.08182j)\n",
      "(-0.63166+0j)\n",
      "(0.31281+0.57941j)\n",
      "(0.31281-0.57941j)\n",
      "(0.40227+0.51394j)\n",
      "(0.40227-0.51394j)\n",
      "(-0.07361+0.64264j)\n",
      "(-0.07361-0.64264j)\n",
      "(0.62195+0.20542j)\n",
      "(0.62195-0.20542j)\n",
      "(0.64618+0j)\n",
      "(-0.41862+0.43967j)\n",
      "(-0.41862-0.43967j)\n",
      "(-0.20964+0.57763j)\n",
      "(-0.20964-0.57763j)\n",
      "(0.54478+0.28894j)\n",
      "(0.54478-0.28894j)\n",
      "(-0.46473+0.35082j)\n",
      "(-0.46473-0.35082j)\n",
      "(-0.46872+0.23534j)\n",
      "(-0.46872-0.23534j)\n",
      "(0.20855+0.49915j)\n",
      "(0.20855-0.49915j)\n",
      "(-0.27675+0.40925j)\n",
      "(-0.27675-0.40925j)\n",
      "(-0.42859+0j)\n",
      "(-0.03274+0.48543j)\n",
      "(-0.03274-0.48543j)\n",
      "(0.48434+0.07829j)\n",
      "(0.48434-0.07829j)\n",
      "(0.10762+0.4477j)\n",
      "(0.10762-0.4477j)\n",
      "(-0.40762+0.11322j)\n",
      "(-0.40762-0.11322j)\n",
      "(0.4043+0.20061j)\n",
      "(0.4043-0.20061j)\n",
      "(-0.16143+0.38871j)\n",
      "(-0.16143-0.38871j)\n",
      "(0.03281+0.41939j)\n",
      "(0.03281-0.41939j)\n",
      "(0.24871+0.33434j)\n",
      "(0.24871-0.33434j)\n",
      "(-0.2966+0.26785j)\n",
      "(-0.2966-0.26785j)\n",
      "(0.4164+0.02625j)\n",
      "(0.4164-0.02625j)\n",
      "(-0.05035+0.39149j)\n",
      "(-0.05035-0.39149j)\n",
      "(-0.25007+0.28559j)\n",
      "(-0.25007-0.28559j)\n",
      "(0.32877+0.20244j)\n",
      "(0.32877-0.20244j)\n",
      "(-0.36414+0.04497j)\n",
      "(-0.36414-0.04497j)\n",
      "(0.05087+0.36504j)\n",
      "(0.05087-0.36504j)\n",
      "(0.17464+0.32916j)\n",
      "(0.17464-0.32916j)\n",
      "(0.2902+0.23023j)\n",
      "(0.2902-0.23023j)\n",
      "(0.24376+0.26046j)\n",
      "(0.24376-0.26046j)\n",
      "(-0.11742+0.29407j)\n",
      "(-0.11742-0.29407j)\n",
      "(-0.29902+0j)\n",
      "(-0.28939+0.07435j)\n",
      "(-0.28939-0.07435j)\n",
      "(0.25345+0.21302j)\n",
      "(0.25345-0.21302j)\n",
      "(-0.01914+0.30062j)\n",
      "(-0.01914-0.30062j)\n",
      "(-0.20406+0.20832j)\n",
      "(-0.20406-0.20832j)\n",
      "(0.21973+0.21838j)\n",
      "(0.21973-0.21838j)\n",
      "(0.05839+0.28538j)\n",
      "(0.05839-0.28538j)\n",
      "(-0.26012+0.07885j)\n",
      "(-0.26012-0.07885j)\n",
      "(-0.11831+0.24544j)\n",
      "(-0.11831-0.24544j)\n",
      "(0.09831+0.26404j)\n",
      "(0.09831-0.26404j)\n",
      "(-0.23364+0.10849j)\n",
      "(-0.23364-0.10849j)\n",
      "(0.30297+0.06088j)\n",
      "(0.30297-0.06088j)\n",
      "(-0.2025+0.15152j)\n",
      "(-0.2025-0.15152j)\n",
      "(-0.23626+0j)\n",
      "(-0.088+0.23781j)\n",
      "(-0.088-0.23781j)\n",
      "(0.26126+0.11602j)\n",
      "(0.26126-0.11602j)\n",
      "(0.08742+0.24148j)\n",
      "(0.08742-0.24148j)\n",
      "(0.27498+0.07273j)\n",
      "(0.27498-0.07273j)\n",
      "(0.06052+0.23516j)\n",
      "(0.06052-0.23516j)\n",
      "(0.16609+0.18656j)\n",
      "(0.16609-0.18656j)\n",
      "(-0.21774+0j)\n",
      "(0.26618+0.01194j)\n",
      "(0.26618-0.01194j)\n",
      "(0.1188+0.20074j)\n",
      "(0.1188-0.20074j)\n",
      "(-0.1293+0.18606j)\n",
      "(-0.1293-0.18606j)\n",
      "(-0.16621+0.14769j)\n",
      "(-0.16621-0.14769j)\n",
      "(0.21759+0.11674j)\n",
      "(0.21759-0.11674j)\n",
      "(-0.14062+0.1432j)\n",
      "(-0.14062-0.1432j)\n",
      "(0.22801+0.04139j)\n",
      "(0.22801-0.04139j)\n",
      "(0.20783+0.09365j)\n",
      "(0.20783-0.09365j)\n",
      "(-0.08534+0.18593j)\n",
      "(-0.08534-0.18593j)\n",
      "(0.21603+0.00835j)\n",
      "(0.21603-0.00835j)\n",
      "(-0.20196+0j)\n",
      "(-0.17839+0.09163j)\n",
      "(-0.17839-0.09163j)\n",
      "(-0.19102+0.04792j)\n",
      "(-0.19102-0.04792j)\n",
      "(-0.05621+0.19321j)\n",
      "(-0.05621-0.19321j)\n",
      "(-0.0322+0.1916j)\n",
      "(-0.0322-0.1916j)\n",
      "(-0.0063+0.1923j)\n",
      "(-0.0063-0.1923j)\n",
      "(0.10498+0.17412j)\n",
      "(0.10498-0.17412j)\n",
      "(0.04556+0.19107j)\n",
      "(0.04556-0.19107j)\n",
      "(0.16761+0.1196j)\n",
      "(0.16761-0.1196j)\n",
      "(0.08944+0.17312j)\n",
      "(0.08944-0.17312j)\n",
      "(-0.10395+0.13703j)\n",
      "(-0.10395-0.13703j)\n",
      "(0.19379+0j)\n",
      "(-0.15355+0.05633j)\n",
      "(-0.15355-0.05633j)\n",
      "(0.04236+0.17686j)\n",
      "(0.04236-0.17686j)\n",
      "(0.17251+0.08964j)\n",
      "(0.17251-0.08964j)\n",
      "(0.13961+0.1213j)\n",
      "(0.13961-0.1213j)\n",
      "(0.06224+0.1659j)\n",
      "(0.06224-0.1659j)\n",
      "(-0.08848+0.12917j)\n",
      "(-0.08848-0.12917j)\n",
      "(-0.13511+0.06989j)\n",
      "(-0.13511-0.06989j)\n",
      "(0.17491+0.0715j)\n",
      "(0.17491-0.0715j)\n",
      "(0.04713+0.16268j)\n",
      "(0.04713-0.16268j)\n",
      "(0.17949+0.02865j)\n",
      "(0.17949-0.02865j)\n",
      "(0.11733+0.11617j)\n",
      "(0.11733-0.11617j)\n",
      "(-0.15003+0j)\n",
      "(-0.02483+0.15018j)\n",
      "(-0.02483-0.15018j)\n",
      "(0.16909+0j)\n",
      "(0.15912+0.03393j)\n",
      "(0.15912-0.03393j)\n",
      "(0.13132+0.08433j)\n",
      "(0.13132-0.08433j)\n",
      "(0.13336+0.07592j)\n",
      "(0.13336-0.07592j)\n",
      "(-0.09573+0.11149j)\n",
      "(-0.09573-0.11149j)\n",
      "(-0.13585+0j)\n",
      "(-0.12873+0.04109j)\n",
      "(-0.12873-0.04109j)\n",
      "(-0.10945+0.08143j)\n",
      "(-0.10945-0.08143j)\n",
      "(-0.04556+0.13583j)\n",
      "(-0.04556-0.13583j)\n",
      "(-0.01377+0.14018j)\n",
      "(-0.01377-0.14018j)\n",
      "(0.01604+0.13945j)\n",
      "(0.01604-0.13945j)\n",
      "(0.072+0.12342j)\n",
      "(0.072-0.12342j)\n",
      "(0.1028+0.09831j)\n",
      "(0.1028-0.09831j)\n",
      "(0.08217+0.11156j)\n",
      "(0.08217-0.11156j)\n",
      "(0.127+0.05486j)\n",
      "(0.127-0.05486j)\n",
      "(0.14039+0.0182j)\n",
      "(0.14039-0.0182j)\n",
      "(0.10973+0.08176j)\n",
      "(0.10973-0.08176j)\n",
      "(0.01825+0.13114j)\n",
      "(0.01825-0.13114j)\n",
      "(0.13205+0.02557j)\n",
      "(0.13205-0.02557j)\n",
      "(-0.06747+0.10346j)\n",
      "(-0.06747-0.10346j)\n",
      "(0.11354+0.06165j)\n",
      "(0.11354-0.06165j)\n",
      "(0.03227+0.12247j)\n",
      "(0.03227-0.12247j)\n",
      "(-0.11365+0j)\n",
      "(-0.10611+0.03343j)\n",
      "(-0.10611-0.03343j)\n",
      "(0.12765+0j)\n",
      "(-0.10454+0.01221j)\n",
      "(-0.10454-0.01221j)\n",
      "(0.01114+0.11504j)\n",
      "(0.01114-0.11504j)\n",
      "(-0.0693+0.08352j)\n",
      "(-0.0693-0.08352j)\n",
      "(-0.08756+0.0565j)\n",
      "(-0.08756-0.0565j)\n",
      "(0.04192+0.10522j)\n",
      "(0.04192-0.10522j)\n",
      "(-0.0137+0.10572j)\n",
      "(-0.0137-0.10572j)\n",
      "(-0.04921+0.08903j)\n",
      "(-0.04921-0.08903j)\n",
      "(0.05809+0.09151j)\n",
      "(0.05809-0.09151j)\n",
      "(0.11071+0.01309j)\n",
      "(0.11071-0.01309j)\n",
      "(-0.09857+0j)\n",
      "(-0.09292+0.03439j)\n",
      "(-0.09292-0.03439j)\n",
      "(-0.06523+0.0721j)\n",
      "(-0.06523-0.0721j)\n",
      "(-0.04828+0.08358j)\n",
      "(-0.04828-0.08358j)\n",
      "(-0.03616+0.08978j)\n",
      "(-0.03616-0.08978j)\n",
      "(-0.06418+0.06485j)\n",
      "(-0.06418-0.06485j)\n",
      "(-0.01533+0.09303j)\n",
      "(-0.01533-0.09303j)\n",
      "(0.10806+0j)\n",
      "(0.08519+0.06368j)\n",
      "(0.08519-0.06368j)\n",
      "(-0.063+0.05173j)\n",
      "(-0.063-0.05173j)\n",
      "(0.10301+0j)\n",
      "(-0.07711+0j)\n",
      "(0.09899+0.02288j)\n",
      "(0.09899-0.02288j)\n",
      "(0.09232+0.04501j)\n",
      "(0.09232-0.04501j)\n",
      "(0.01196+0.09155j)\n",
      "(0.01196-0.09155j)\n",
      "(0.05958+0.07803j)\n",
      "(0.05958-0.07803j)\n",
      "(0.00768+0.08985j)\n",
      "(0.00768-0.08985j)\n",
      "(0.039+0.08522j)\n",
      "(0.039-0.08522j)\n",
      "(0.08983+0.04251j)\n",
      "(0.08983-0.04251j)\n",
      "(0.06113+0.07204j)\n",
      "(0.06113-0.07204j)\n",
      "(-0.0746+0.02487j)\n",
      "(-0.0746-0.02487j)\n",
      "(0.08555+0.04114j)\n",
      "(0.08555-0.04114j)\n",
      "(-0.00954+0.08504j)\n",
      "(-0.00954-0.08504j)\n",
      "(0.08923+0.01959j)\n",
      "(0.08923-0.01959j)\n",
      "(0.03708+0.0792j)\n",
      "(0.03708-0.0792j)\n",
      "(-0.05864+0.04583j)\n",
      "(-0.05864-0.04583j)\n",
      "(0.064+0.05507j)\n",
      "(0.064-0.05507j)\n",
      "(0.04344+0.06947j)\n",
      "(0.04344-0.06947j)\n",
      "(-0.00064+0.07774j)\n",
      "(-0.00064-0.07774j)\n",
      "(0.02471+0.0738j)\n",
      "(0.02471-0.0738j)\n",
      "(-0.06765+0.00875j)\n",
      "(-0.06765-0.00875j)\n",
      "(-0.06539+0.01543j)\n",
      "(-0.06539-0.01543j)\n",
      "(-0.04989+0.05107j)\n",
      "(-0.04989-0.05107j)\n",
      "(-0.05745+0.02991j)\n",
      "(-0.05745-0.02991j)\n",
      "(-0.02993+0.06193j)\n",
      "(-0.02993-0.06193j)\n",
      "(-0.00768+0.07135j)\n",
      "(-0.00768-0.07135j)\n",
      "(-0.04743+0.0417j)\n",
      "(-0.04743-0.0417j)\n",
      "(0.03682+0.06545j)\n",
      "(0.03682-0.06545j)\n",
      "(-0.01982+0.06353j)\n",
      "(-0.01982-0.06353j)\n",
      "(-0.01218+0.0646j)\n",
      "(-0.01218-0.0646j)\n",
      "(0.07438+0.01796j)\n",
      "(0.07438-0.01796j)\n",
      "(0.05908+0.04613j)\n",
      "(0.05908-0.04613j)\n",
      "(0.06859+0.03145j)\n",
      "(0.06859-0.03145j)\n",
      "(-0.03977+0.04083j)\n",
      "(-0.03977-0.04083j)\n",
      "(-0.02978+0.04898j)\n",
      "(-0.02978-0.04898j)\n",
      "(0.0152+0.06339j)\n",
      "(0.0152-0.06339j)\n",
      "(0.06884+0.00717j)\n",
      "(0.06884-0.00717j)\n",
      "(0.01898+0.0595j)\n",
      "(0.01898-0.0595j)\n",
      "(0.06108+0j)\n",
      "(0.05755+0.02926j)\n",
      "(0.05755-0.02926j)\n",
      "(0.00412+0.06294j)\n",
      "(0.00412-0.06294j)\n",
      "(0.03649+0.04936j)\n",
      "(0.03649-0.04936j)\n",
      "(0.05348+0.03318j)\n",
      "(0.05348-0.03318j)\n",
      "(0.04786+0.03557j)\n",
      "(0.04786-0.03557j)\n",
      "(-0.05765+0j)\n",
      "(-0.05168+0.01694j)\n",
      "(-0.05168-0.01694j)\n",
      "(-0.01069+0.05527j)\n",
      "(-0.01069-0.05527j)\n",
      "(-0.00319+0.05648j)\n",
      "(-0.00319-0.05648j)\n",
      "(-0.04415+0.02257j)\n",
      "(-0.04415-0.02257j)\n",
      "(-0.0325+0.03681j)\n",
      "(-0.0325-0.03681j)\n",
      "(-0.04669+0.01172j)\n",
      "(-0.04669-0.01172j)\n",
      "(0.0606+0.01462j)\n",
      "(0.0606-0.01462j)\n",
      "(-0.046+0j)\n",
      "(-0.04529+0j)\n",
      "(-0.04185+0.0112j)\n",
      "(-0.04185-0.0112j)\n",
      "(-0.03392+0.02574j)\n",
      "(-0.03392-0.02574j)\n",
      "(-0.03034+0.0275j)\n",
      "(-0.03034-0.0275j)\n",
      "(0.0598+0j)\n",
      "(0.03097+0.0449j)\n",
      "(0.03097-0.0449j)\n",
      "(0.00472+0.04887j)\n",
      "(0.00472-0.04887j)\n",
      "(-0.01745+0.03618j)\n",
      "(-0.01745-0.03618j)\n",
      "(0.03497+0.03959j)\n",
      "(0.03497-0.03959j)\n",
      "(0.04734+0.0233j)\n",
      "(0.04734-0.0233j)\n",
      "(-0.0082+0.03926j)\n",
      "(-0.0082-0.03926j)\n",
      "(0.05126+0.01124j)\n",
      "(0.05126-0.01124j)\n",
      "(0.04125+0j)\n",
      "(0.00111+0.04311j)\n",
      "(0.00111-0.04311j)\n",
      "(0.03466+0.03378j)\n",
      "(0.03466-0.03378j)\n",
      "(0.04714+0.01302j)\n",
      "(0.04714-0.01302j)\n",
      "(0.02201+0.03941j)\n",
      "(0.02201-0.03941j)\n",
      "(0.01274+0.04106j)\n",
      "(0.01274-0.04106j)\n",
      "(0.01693+0.03714j)\n",
      "(0.01693-0.03714j)\n",
      "(0.02424+0.03444j)\n",
      "(0.02424-0.03444j)\n",
      "(0.03884+0.01731j)\n",
      "(0.03884-0.01731j)\n",
      "(0.03685+0.01826j)\n",
      "(0.03685-0.01826j)\n",
      "(0.03923+0.00757j)\n",
      "(0.03923-0.00757j)\n",
      "(-0.03302+0.0058j)\n",
      "(-0.03302-0.0058j)\n",
      "(-0.01755+0.02765j)\n",
      "(-0.01755-0.02765j)\n",
      "(-0.01267+0.03055j)\n",
      "(-0.01267-0.03055j)\n",
      "(0.03953+0j)\n",
      "(-0.0036+0.03323j)\n",
      "(-0.0036-0.03323j)\n",
      "(0.00542+0.03316j)\n",
      "(0.00542-0.03316j)\n",
      "(0.01577+0.03091j)\n",
      "(0.01577-0.03091j)\n",
      "(0.02653+0.0198j)\n",
      "(0.02653-0.0198j)\n",
      "(0.01969+0.02487j)\n",
      "(0.01969-0.02487j)\n",
      "(0.02959+0.00859j)\n",
      "(0.02959-0.00859j)\n",
      "(-0.01403+0.02506j)\n",
      "(-0.01403-0.02506j)\n",
      "(-0.02483+0.01109j)\n",
      "(-0.02483-0.01109j)\n",
      "(-0.01836+0.01996j)\n",
      "(-0.01836-0.01996j)\n",
      "(-0.02338+0j)\n",
      "(0.01908+0.02161j)\n",
      "(0.01908-0.02161j)\n",
      "(-0.01363+0.01888j)\n",
      "(-0.01363-0.01888j)\n",
      "(-0.00333+0.02609j)\n",
      "(-0.00333-0.02609j)\n",
      "(-0.02195+0.00506j)\n",
      "(-0.02195-0.00506j)\n",
      "(-0.00017+0.02498j)\n",
      "(-0.00017-0.02498j)\n",
      "(-0.01698+0.01236j)\n",
      "(-0.01698-0.01236j)\n",
      "(0.00715+0.0233j)\n",
      "(0.00715-0.0233j)\n",
      "(0.02164+0.01492j)\n",
      "(0.02164-0.01492j)\n",
      "(-0.0201+0j)\n",
      "(0.02275+0.00788j)\n",
      "(0.02275-0.00788j)\n",
      "(-0.00655+0.01855j)\n",
      "(-0.00655-0.01855j)\n",
      "(0.02183+0.00355j)\n",
      "(0.02183-0.00355j)\n",
      "(0.01611+0.01542j)\n",
      "(0.01611-0.01542j)\n",
      "(0.01113+0.01767j)\n",
      "(0.01113-0.01767j)\n",
      "(0.01772+0.0085j)\n",
      "(0.01772-0.0085j)\n",
      "(-0.01638+0.00182j)\n",
      "(-0.01638-0.00182j)\n",
      "(0.00602+0.01485j)\n",
      "(0.00602-0.01485j)\n",
      "(-0.00201+0.01447j)\n",
      "(-0.00201-0.01447j)\n",
      "(-0.0078+0.01173j)\n",
      "(-0.0078-0.01173j)\n",
      "(-0.01419+0j)\n",
      "(-0.00548+0.01225j)\n",
      "(-0.00548-0.01225j)\n",
      "(0.00687+0.01178j)\n",
      "(0.00687-0.01178j)\n",
      "(0.00165+0.01353j)\n",
      "(0.00165-0.01353j)\n",
      "(-0.01078+0.00673j)\n",
      "(-0.01078-0.00673j)\n",
      "(0.01312+0.00574j)\n",
      "(0.01312-0.00574j)\n",
      "(0.01309+0j)\n",
      "(0.01131+0.00455j)\n",
      "(0.01131-0.00455j)\n",
      "(0.01186+0j)\n",
      "(-0.0077+0.00585j)\n",
      "(-0.0077-0.00585j)\n",
      "(-0.01079+0j)\n",
      "(0.00265+0.00907j)\n",
      "(0.00265-0.00907j)\n",
      "(-0.00734+0.00334j)\n",
      "(-0.00734-0.00334j)\n",
      "(0.00686+0.00339j)\n",
      "(0.00686-0.00339j)\n",
      "(-0.00379+0.00536j)\n",
      "(-0.00379-0.00536j)\n",
      "(0.0006+0.00612j)\n",
      "(0.0006-0.00612j)\n",
      "(0.00872+0j)\n",
      "(0.00306+0.00441j)\n",
      "(0.00306-0.00441j)\n",
      "(-0.00399+0j)\n",
      "(-0.00255+0.00288j)\n",
      "(-0.00255-0.00288j)\n",
      "(0.0042+0j)\n",
      "(-0.00073+0.00099j)\n",
      "(-0.00073-0.00099j)\n",
      "(0.00099+0j)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65659/3746453529.py:2: DeprecationWarning: The Python built-in `round` is deprecated for complex scalars, and will raise a `TypeError` in a future release. Use `np.round` or `scalar.round` instead.\n",
      "  print(round(i,5))\n"
     ]
    }
   ],
   "source": [
    "for i in w:\n",
    "    print(round(i,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "853f23fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-271.6411318751175-7.714377189984287j) (260.30649003419984+0j) 56.589194459364066\n"
     ]
    }
   ],
   "source": [
    "print(min(w), max(w), np.std(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3fde7d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09238127581293652-3.9619119074872644e-17j)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5aa0dc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([115846, 1024])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bd129a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([115846, 1024])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e447d484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([715, 246, 247,  ..., 986, 968, 674])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "10d66454",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_texts = np.matmul(final_texts.cpu(), upsampled2_balanced_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fe251f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.58209378326274"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(projected_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2ef55dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339.276"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(final_images.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "40038655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339.25485"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(final_texts.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "07ca026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =      1025000     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  8.00236D+05    |proj g|=  1.16415D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.66757D+05    |proj g|=  5.40155D+01\n",
      "\n",
      "At iterate  100    f=  2.66095D+05    |proj g|=  1.68676D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "*****    100    106      1     0     0   1.687D+00   2.661D+05\n",
      "  F =   266095.48310057109     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/bdevnani3/flash1/miniconda3/envs/ltr/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 36.5min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0,verbose=1, n_jobs=-1).fit(projected_texts, final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "93b3a109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.894748200196813"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(projected_texts, final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d1f8368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =      1025000     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  8.00236D+05    |proj g|=  1.16415D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  1.31944D+05    |proj g|=  4.53975D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "*****     97    100      1     0     0   3.988D-02   1.319D+05\n",
      "  F =   131928.92824115991     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 49.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7241423959394369"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0,verbose=1, n_jobs=-1).fit(final_texts.cpu(), final_labels)\n",
    "clf.score(projected_texts, final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5e32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
